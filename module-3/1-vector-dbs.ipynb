{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.2\n"
     ]
    }
   ],
   "source": [
    "! python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! pip install llama-index\n",
    "! pip install llama-index-vector-stores-postgres\n",
    "! pip install psycopg2-binary\n",
    "! pip install openai\n",
    "! pip install llama-index-embeddings-huggingface\n",
    "! pip install ipywidgets\n",
    "! pip install SQLAlchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the PostgresSQL database\n",
    "\n",
    "After setting up the Postgres database as described in the README, we can connect to the database using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import nest_asyncio\n",
    "\n",
    "try:\n",
    "    pg_pw = \"mysecretpassword\"\n",
    "    pg_db = \"vector_store\"\n",
    "    connection_string = f\"postgresql://postgres:{pg_pw}@localhost:5432\"\n",
    "    db_name = pg_db\n",
    "    conn = psycopg2.connect(connection_string)\n",
    "    conn.autocommit = True\n",
    "\n",
    "    with conn.cursor() as c:\n",
    "        c.execute(f\"DROP DATABASE {db_name} WITH (FORCE);\")\n",
    "        c.execute(f\"CREATE DATABASE {db_name};\")\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    nest_asyncio.apply()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access embedding models using the OpenAI API\n",
    "\n",
    "As with any model hosted in LM Studio, we can access the embedding model using the OpenAI API. We can use the following code to access the model. Note how the embeddings look like and how many dimensions are in them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03201862424612045, 0.06339021772146225, -0.1485486477613449, -0.07495227456092834, 0.05322074145078659]\n",
      "Emedding length: 768\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "def get_embedding(text, model=\"nomic-ai/nomic-embed-text-v1.5-GGUF\"):\n",
    "   \"\"\"\n",
    "   Get embeddings using LM Studio (Open AI API)\n",
    "   \"\"\" \n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   return client.embeddings.create(input = [text], model=model).data[0].embedding\n",
    "\n",
    "\n",
    "embeddings = get_embedding(\"Once upon a time, there was a cat.\")\n",
    "print(embeddings[:5])\n",
    "print(f\"Emedding length: {len(embeddings)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using any of the HuggingFace embedding models\n",
    "\n",
    "To easily use embedding models in LlamaIndex, we can use any of the embedding models available in [Hugging Face](https://huggingface.co/models?other=embeddings&sort=downloads), load them in memory and use them for your embeddings. This way, you will not have to incur any costs for using the OpenAI API.\n",
    "\n",
    "Other alternatives are to use umbedding models hosted by the likes of OpenAI, Meta, Google, and others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.04875882342457771, -0.04734064266085625, 0.020610108971595764, 0.02316340245306492, 0.04693278670310974]\n",
      "Emedding length: 768\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "\"\"\"\n",
    "Get embeddings using Hugging Face embeddings (can be local or remote)\n",
    "Note: we use this because the OpenAI API embedding model cannot be used in Llama Index\n",
    "\"\"\"\n",
    "\n",
    "model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "text_embedding = model.get_text_embedding(\"Once upon a time, there was a cat.\")\n",
    "print(text_embedding[:5])\n",
    "print(f\"Emedding length: {len(text_embedding)}\")\n",
    "vector_size = len(text_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingesting and creating embeddings for your data\n",
    "\n",
    "Here we are using LlamaIndex SimpleDirectoryReader to read the data from the directory and create embeddings for the data. We can use the following code to create embeddings for the data.\n",
    "\n",
    "Note that we are we have specified the chunking size and overlap to use for the embeddings. We have used `PGVectorStore` which represents the vector DB that we have just setup in the Postgres database. Then running `VectorStoreIndex.from_documents` is actually what ingests the data and creates the embeddings for the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from sqlalchemy import make_url\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.vector_stores.postgres import PGVectorStore\n",
    "\n",
    "Settings.embed_model = model\n",
    "Settings.chunk_size = 1024\n",
    "Settings.chunk_overlap = 20\n",
    "\n",
    "BASE_DIR = \"./data\"\n",
    "\n",
    "def simple_RAG(vector_size):\n",
    "    \"\"\"\n",
    "    Simple Retrieval Augmented Generation (RAG) using Llama Index.\n",
    "    \"\"\"\n",
    "\n",
    "    url = make_url(connection_string)\n",
    "    print(f\"Url {url}\")\n",
    "    \n",
    "    vector_store = PGVectorStore.from_params(\n",
    "        database=db_name,\n",
    "        host=url.host,\n",
    "        password=url.password,\n",
    "        port=url.port,\n",
    "        user=url.username,\n",
    "        table_name=\"simple_rag\",\n",
    "        embed_dim=vector_size\n",
    "    )\n",
    "\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    \n",
    "    nodes = ingest_documents(BASE_DIR)\n",
    "    \n",
    "    print(f\"Number of nodes: {len(nodes)}\")\n",
    "    print()\n",
    "    for node in nodes:\n",
    "        print(f\"Node text: {node.text}\")\n",
    "        print(f\"Node length: {len(node.text)}\")\n",
    "        print()\n",
    "\n",
    "    index = VectorStoreIndex.from_documents(nodes, storage_context=storage_context, show_progress=True)\n",
    "    return index\n",
    "\n",
    "def ingest_documents(directory):\n",
    "    \"\"\"\n",
    "    Ingest documents from a directory into the vector store. \n",
    "    \"\"\"\n",
    "    reader = SimpleDirectoryReader(input_dir=directory)\n",
    "    return reader.load_data(show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's run our document ingestion code\n",
    "\n",
    "Now that we have all the code in place, let's run the code to ingest the data and create embeddings for the data with the code below. Notice how we've enabled the `verbose` flag to see the progress of the ingestion process.\n",
    "\n",
    "After this, let's have a look at the embeddings that we have created for the data, by logging into the DB using our SQL client. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Url postgresql://postgres:***@localhost:5432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|██████████| 1/1 [00:00<00:00, 69.95file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 4\n",
      "\n",
      "Node text: \n",
      "\n",
      "Code.Sydney and Generative AI\n",
      "\n",
      "Here in Code.Sydney, we are building a **learning** and **collaborative** community around technologies such as web, mobile, data, and in this latest initiative, Generative AI.\n",
      "\n",
      "Node length: 210\n",
      "\n",
      "Node text: \n",
      "\n",
      "Generative AI Building Blocks\n",
      "\n",
      "In 2024, the top Generative use cases according to AWS:\n",
      "\n",
      "- Knowledge base search & chatbots\n",
      "- Contact Center Intelligence + GenAI\n",
      "- Intelligent Document Processing + GenAI\n",
      "- Personalization + GenAI\n",
      "\n",
      "If you look at following image, you can see that **Knowledge base search & chatbots** is the top use case for Generative AI. The tech landscape is changing rapidly and we need to keep up with the latest trends. Instead of tackling all these use cases, it is better to focus on one use case and become an expert in that area.\n",
      "\n",
      "!top Generative use cases\n",
      "\n",
      "So, for the next few months, Code.Sydney will be focusing on learning the building blocks of Generative AI to enable us to build solutions in the **Knowledge base search & chatbots** area.\n",
      "\n",
      "This will be a progressive learning journey where we will start with the basics and gradually build up our knowledge and skills in Generative AI, with the aim of enabling our members to build their own end to end Generative AI solutions.\n",
      "\n",
      "Node length: 1013\n",
      "\n",
      "Node text: \n",
      "\n",
      "Generative AI Learning Path\n",
      "\n",
      "We will be following the learning path below:\n",
      "\n",
      "**Module 1** Fundamentals\n",
      "- Software Development Fundamentals\n",
      "- Git Basics\n",
      "- Python Basics\n",
      "- Jupyter Notebooks\n",
      "- Preparing your Python environment\n",
      "- Introduction to Generative AI Learning Path\n",
      "\n",
      "**Module 2** Generative AI Basics\n",
      "- Introduction to Generative AI\n",
      "- Access to LLM using Python\n",
      "- Online vs Local LLM\n",
      "- Setting up your local LLM\n",
      "- Generative AI Libraries (eg. LangChain and LlamaIndex)\n",
      "\n",
      "**Practical Project** Local LLM Mini project\n",
      "- Setting up your Local LLM\n",
      "- Build a simple chatbot using Local LLM\n",
      "- Deliverables:\n",
      "    - Jupyter notebook chatbot using LangChain and another one using LlamaIndex\n",
      "    - Documentation on how to setup Local LLM and run the chatbot\n",
      "    - personal Github repo with your chatbot project\n",
      "    - Assume you are presenting this to a client\n",
      "\n",
      "**Module 3** Vector Databases\n",
      "- Introduction to Vector Databases\n",
      "- Get to know 2 popular Vector Databases (PgVector and ChromaDB)\n",
      "- Setting up PgVector and ChromaDB\n",
      "- Accessing PgVector and ChromaDB using LlamaIndex or LangChain\n",
      "- Ingestion of your dataset\n",
      "\n",
      "**Module 4** Retrieval-Augmented Generation (RAG)\n",
      "- Introduction to RAG\n",
      "- Basic RAG in LlamaIndex and LangChain\n",
      "- Chunking strategies\n",
      "- Embedding model selection\n",
      "\n",
      "**Practical Project** RAG Mini project\n",
      "- Build a simple RAG model using LlamaIndex and LangChain\n",
      "\n",
      "**Module 5** Advanced RAG\n",
      "- RAG failure modes\n",
      "- Advanced RAG strategies\n",
      "- Metadata filtering and RAG\n",
      "- RAG with multiple documents\n",
      "\n",
      "**Practical Project** Advanced RAG project\n",
      "- Build a Chatbot that works for Complex queries and against multiple documents\n",
      "\n",
      "**Module 6** Frontend options\n",
      "- Introduction to Frontend options\n",
      "- StreamLit\n",
      "- NextJS\n",
      "- ReactJS\n",
      "\n",
      "**Module 7** Backend options\n",
      "- Introduction to Backend options\n",
      "- Flask\n",
      "- FastAPI\n",
      "- DJango\n",
      "\n",
      "**Final Project** End to End Generative AI project\n",
      "- document ingestion pipeline\n",
      "- FrontEnd + Backend + Local LLM + Vector Database + RAG\n",
      "\n",
      "Node length: 1955\n",
      "\n",
      "Node text: \n",
      "\n",
      "Environment setup\n",
      "- use `pyenv` to manage python versions\n",
      "- use `venv` to manage your virtual environments\n",
      "\n",
      "```bash\n",
      "pyenv versions\n",
      "pyenv install 3.12.2\n",
      "pyenv local 3.12.2\n",
      "python -m venv .venv\n",
      "source .venv/bin/activate\n",
      "``` \n",
      "Node length: 224\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1374fb4e246c483baa03cdd551524a79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7bced8914dd442387fc725688c06d09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retriever = simple_RAG(vector_size=vector_size).as_retriever(similarity_top_k=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 1\n",
      "\n",
      "Node text: Code.Sydney and Generative AI\n",
      "\n",
      "Here in Code.Sydney, we are building a **learning** and **collaborative** community around technologies such as web, mobile, data, and in this latest initiative, Generative AI.\n",
      "Score: 0.67157\n",
      "Node length: 207\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nodes = retriever.retrieve(\"What does Code.Sydney do?\")\n",
    "\n",
    "print(f\"Number of nodes: {len(nodes)}\")\n",
    "print()\n",
    "for node in nodes:\n",
    "    print(f\"Node text: {node.text}\")\n",
    "    print(f\"Score: {round(node.score, 5)}\")\n",
    "    print(f\"Node length: {len(node.text)}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
